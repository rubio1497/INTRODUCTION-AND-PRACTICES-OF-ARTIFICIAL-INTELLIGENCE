{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wbIgG7sWXA2N"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "# 圖片像素\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "# 插值\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LvurbAOnXcpQ"
   },
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3rqP1VZAZDRM"
   },
   "source": [
    "↓ 這裡使用的dataset取樣自MNIST,\n",
    "+  x:(?, 28,28) unsigned int\n",
    "+  y:(?, 1) unsigned int 表為奇數還偶數\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EQzRxXeyXZ6H"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1ol0M43I9MfC4qECiUnXVT-jYPgPiGiFH\n",
      "To: C:\\Users\\Rubio\\AppData\\Roaming\\SPB_16.6\\data.npz\n",
      "\n",
      "  0%|          | 0.00/67.0k [00:00<?, ?B/s]\n",
      "100%|██████████| 67.0k/67.0k [00:00<00:00, 3.78MB/s]\n"
     ]
    }
   ],
   "source": [
    "# get dataset\n",
    "!gdown --id \"1ol0M43I9MfC4qECiUnXVT-jYPgPiGiFH\" --output data.npz\n",
    "loader = np.load('data.npz')\n",
    "x_train_orig, y_train_orig, x_test_orig, y_test_orig = loader.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rjDcTUovafFC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: \t(300, 28, 28)\n",
      "train label shape: \t(300,)\n",
      "test data shape: \t(100, 28, 28)\n",
      "test label shape: \t(100,)\n"
     ]
    }
   ],
   "source": [
    "# check data shape\n",
    "print('train data shape: \\t' + str(x_train_orig.shape))\n",
    "print('train label shape: \\t' + str(y_train_orig.shape))\n",
    "print('test data shape: \\t' + str(x_test_orig.shape))\n",
    "print('test label shape: \\t' + str(y_test_orig.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WBWk_6AgcIPX"
   },
   "source": [
    "↑  預期輸出:\n",
    "```\n",
    "train data shape: \t(300, 28, 28) -> 代表有300張圖片訓練\n",
    "train label shape: \t(300,)\n",
    "test data shape: \t(100, 28, 28)\n",
    "test label shape: \t(100,)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ri2W9KFndUIM"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is an even number\n"
     ]
    }
   ],
   "source": [
    "# Example of a picture\n",
    "# 更改 index 可得其他圖片\n",
    "index = 3\n",
    "# imshow 顯示圖片\n",
    "plt.imshow(x_train_orig[index])\n",
    "plt.show()\n",
    "LABEL = ['odd number', 'even number']\n",
    "print('this is an ' + LABEL[y_train_orig[index]] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tQRKGV5Th0tA"
   },
   "source": [
    "↑  預期輸出:\\\n",
    "![](https://i.imgur.com/mMkP92G.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is an odd number\n"
     ]
    }
   ],
   "source": [
    "index = 6\n",
    "# imshow 顯示圖片\n",
    "plt.imshow(x_train_orig[index])\n",
    "plt.show()\n",
    "LABEL = ['odd number', 'even number']\n",
    "print('this is an ' + LABEL[y_train_orig[index]] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is an odd number\n"
     ]
    }
   ],
   "source": [
    "index = 299\n",
    "# imshow 顯示圖片\n",
    "plt.imshow(x_train_orig[index])\n",
    "plt.show()\n",
    "LABEL = ['odd number', 'even number']\n",
    "print('this is an ' + LABEL[y_train_orig[index]] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ikiCqkn_gYWu"
   },
   "source": [
    "# Data Preprocessing\n",
    "↓ 進入MLP前，我們需要先對資料進行處理\n",
    "1.  將圖片攤平(二維轉為單維陣列)，此處僅為對應**全連接層**所需之操作\n",
    "2.  Standardize  Data to [0.0 ~ 1.0]\n",
    "3.  將label轉為1維轉成N維資料(分成N類)(本次不需要這麼做)\n",
    "4.  (本次因後續操作考量，將資料矩陣翻轉)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "62r7ADEigTyf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: \t(784, 300)\n",
      "train label shape: \t(1, 300)\n",
      "test data shape: \t(784, 100)\n",
      "test label shape: \t(1, 100)\n"
     ]
    }
   ],
   "source": [
    "# Reshape the training and test examples \n",
    "# 一般用法：numpy.arange(n).reshape(a, b); 依次生成n個自然数，並且以a行b列的數组形式顯示:\n",
    "# 特殊用法：mat (or array).reshape(c, -1); 必须是矩陣格式或者數组格式，才能使用 .reshape(c, -1) 函數， \n",
    "# 表示將此矩陣或者數组重组，以 c行d列的形式表示（-1的作用就在此，自動計算d：d=數组或者矩陣裡面所有的元素個数/c,\n",
    "# d必须是整數，不然報错）（reshape(-1, e)即列數固定，行數需要計算）：\n",
    "# 其他用法：numpy.arange(a,b,c)/ numpy.arange(a,b,c).reshape(m,n); 從數字a起, 步長為c, 到b结束，生成array：\n",
    "# .T 為 Array Transform\n",
    "x_train_flatten = x_train_orig.reshape(x_train_orig.shape[0], -1).T  \n",
    "# The \"-1\" makes reshape flatten the remaining dimensions\n",
    "x_test_flatten = x_test_orig.reshape(x_test_orig.shape[0], -1).T\n",
    "\n",
    "# Standardize data to have feature values between 0 and 1.\n",
    "x_train = x_train_flatten/255.\n",
    "x_test = x_test_flatten/255.\n",
    "\n",
    "## One-Hot encode the data label, but in this case we don't need to do\n",
    "#y_train = np.eye(N)[y_train_orig].T\n",
    "#y_test = np.eye(N)[y_test_orig].T\n",
    "y_train = y_train_orig.reshape([1,-1])\n",
    "y_test = y_test_orig.reshape([1,-1])\n",
    "\n",
    "# check data shape\n",
    "print('train data shape: \\t' + str(x_train.shape))\n",
    "print('train label shape: \\t' + str(y_train.shape))\n",
    "print('test data shape: \\t' + str(x_test.shape))\n",
    "print('test label shape: \\t' + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_orig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y5_6UKYekTmU"
   },
   "source": [
    "↑  預期輸出:\n",
    "```\n",
    "train data shape: \t(784, 300)\n",
    "train label shape: \t(1, 300)\n",
    "test data shape: \t(784, 100)\n",
    "test label shape: \t(1, 100)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tiTEEmC1kXaa"
   },
   "source": [
    "# Forward and Backward\n",
    "1. forward\\\n",
    "  $\\text{cost} = L(F(w))$\n",
    "2. backward\\\n",
    "  $-\\triangle{w} \\propto \\frac{d\\text{cost}}{dw}$\n",
    "\n",
    "  $\\frac{d\\text{cost}}{dw} = \\frac{dL}{dF}\\frac{dF}{dw}$, (by *Chain Rule*)\n",
    "\n",
    "  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w-ekP6Nxkklb"
   },
   "source": [
    "##  linear function\n",
    "1. forward\\\n",
    " $ z = f(X,W,b) = \\sum_{i=1}^{n}(W_i \\cdot X_i) + b$\n",
    "\n",
    "2. backward\\\n",
    " $ -\\triangle W_i \\propto \\frac{dL}{df} \\cdot X_i $\n",
    "\n",
    " $ -\\triangle dx_i \\propto \\frac{dL}{df} \\cdot W_i $\n",
    "\n",
    " $ -\\triangle db \\propto \\frac{dL}{df} \\cdot 1 $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pW04mMBuojbG"
   },
   "source": [
    "↓ 可參考: [numpy.dot](https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html) , etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZEu8tAODkW67"
   },
   "outputs": [],
   "source": [
    "# linear layer forward propagation 內積型式\n",
    "def linear_forward(X, W, b):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- output from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python dictionary containing \"X\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    ##################################\n",
    "    ####          TODO            ####\n",
    "    ##################################\n",
    "    Z = np.dot(W,X) + b\n",
    "    ##################################\n",
    "    ####          TODO            ####\n",
    "    ##################################\n",
    "\n",
    "    # 如果大小不同，raise exception \n",
    "    assert(Z.shape == (W.shape[0], X.shape[1]))\n",
    "    cache = (X, W, b)\n",
    "    \n",
    "    return Z, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LexaEUBJt9A9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X : [[-1.02690451  0.22174943  1.13039011  1.14618522 -0.59273429]\n",
      " [ 0.11878403 -0.48443021 -1.94491271  0.09207734  0.90216909]\n",
      " [ 1.31446859  0.77110218 -0.54014736 -0.28411518 -0.88933117]]\n",
      "W : [[ 0.40416927 -1.14481162  0.54539594]\n",
      " [ 1.45406998  1.22397676  0.22120243]]\n",
      "b : [[0.]\n",
      " [0.]]\n",
      "Z : [[ 0.16587724  1.06476164  2.38883344  0.20288637 -1.75741625]\n",
      " [-1.05703849 -0.09992247 -0.85634353  1.71648708  0.04563466]]\n"
     ]
    }
   ],
   "source": [
    "# check linear_forward\n",
    "# 製造不同的亂數 108 個\n",
    "np.random.seed(108)\n",
    "\n",
    "num_of_input = 5\n",
    "input_shape = 3\n",
    "output_shape = 2\n",
    "\n",
    "# np.random.randn(x,y) = > x rows , y columns\n",
    "\n",
    "X = np.random.randn(input_shape, num_of_input) \n",
    "W = np.random.randn(output_shape, input_shape)\n",
    "b = np.zeros((output_shape,1))\n",
    "\n",
    "Z, cache = linear_forward(X,W,b)\n",
    "\n",
    "print('X : ' + str(X))\n",
    "print('W : ' + str(W))\n",
    "print('b : ' + str(b))\n",
    "print('Z : ' + str(Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nI7CTSE_1v8D"
   },
   "source": [
    "↑  預期輸出:\n",
    "```\n",
    "X : [[-1.02690451  0.22174943  1.13039011  1.14618522 -0.59273429]\n",
    " [ 0.11878403 -0.48443021 -1.94491271  0.09207734  0.90216909]\n",
    " [ 1.31446859  0.77110218 -0.54014736 -0.28411518 -0.88933117]]\n",
    "W : [[ 0.40416927 -1.14481162  0.54539594]\n",
    " [ 1.45406998  1.22397676  0.22120243]]\n",
    "b : [[0.]\n",
    " [0.]]\n",
    "Z : [[ 0.16587724  1.06476164  2.38883344  0.20288637 -1.75741625]\n",
    " [-1.05703849 -0.09992247 -0.85634353  1.71648708  0.04563466]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Oyb-OcYbyJzs"
   },
   "outputs": [],
   "source": [
    "# linear layer backward propagation\n",
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output\n",
    "    cache -- tuple of values (X_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dX_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as X_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    X_prev, W, b = cache\n",
    "    m = X_prev.shape[-1] # shape 結果後最後一個數字\n",
    "    \n",
    "    dW = 1./m * np.dot(dZ,X_prev.T)\n",
    "    dX_prev = np.dot(W.T,dZ)\n",
    "    db = 1./m * np.sum(dZ, axis = 1, keepdims = True)\n",
    "    \n",
    "    assert (dX_prev.shape == X_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dX_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kH-LwGd8yWRV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dZ : [[-0.18773494  2.44063677 -0.94090784  0.61391165  0.73562667]\n",
      " [ 0.97291789  0.25919495 -0.07407492 -0.17768802 -0.83874355]]\n",
      "dX_prev : [[ 1.338814    1.36331797 -0.48799614 -0.01024659 -0.92227413]\n",
      " [ 1.40575002 -2.47682075  0.98649625 -0.9202992  -1.86875658]\n",
      " [ 0.11282192  1.38844793 -0.52955286  0.2955199   0.21567569]]\n",
      "dW : [[-0.01239439  0.2691105   0.26296004]\n",
      " [-0.14637246 -0.12779501  0.46303036]]\n",
      "db : [[0.53230646]\n",
      " [0.02832127]]\n"
     ]
    }
   ],
   "source": [
    "# check linear_backward\n",
    "np.random.seed(109)\n",
    "dZ = np.random.randn(output_shape, num_of_input)\n",
    "\n",
    "dX_prev, dW, db = linear_backward(dZ, cache)\n",
    "print('dZ : ' + str(dZ))\n",
    "print('dX_prev : ' + str(dX_prev))\n",
    "print('dW : ' + str(dW))\n",
    "print('db : ' + str(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IbEVxtMX13c8"
   },
   "source": [
    "↑  預期輸出:\n",
    "```\n",
    "dZ : [[-0.18773494  2.44063677 -0.94090784  0.61391165  0.73562667]\n",
    " [ 0.97291789  0.25919495 -0.07407492 -0.17768802 -0.83874355]]\n",
    "dX_prev : [[ 1.338814    1.36331797 -0.48799614 -0.01024659 -0.92227413]\n",
    " [ 1.40575002 -2.47682075  0.98649625 -0.9202992  -1.86875658]\n",
    " [ 0.11282192  1.38844793 -0.52955286  0.2955199   0.21567569]]\n",
    "dW : [[-0.01239439  0.2691105   0.26296004]\n",
    " [-0.14637246 -0.12779501  0.46303036]]\n",
    "db : [[0.53230646]\n",
    " [0.02832127]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w3OiVr6Dj4eG"
   },
   "source": [
    "## activation function\n",
    "Sigmoid:\n",
    "1. forward\\\n",
    " $ Y = f(Z) = \\frac{1}{1 + e^{-Z}}$\n",
    "\n",
    "2. backward\\\n",
    " $  -\\triangle Z \\propto \\frac{dL}{dZ} = \\frac{dL}{df} \\cdot f(Z) \\cdot (1-f(Z))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ad1US7JZoqjz"
   },
   "source": [
    "↓ 可參考: [numpy.exp](https://docs.scipy.org/doc/numpy/reference/generated/numpy.exp.html) , etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bMKN-Y84ggNh"
   },
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape\n",
    "    \n",
    "    Returns:\n",
    "    Y -- output of sigmoid(z), same shape as Z\n",
    "    cache -- returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "    \n",
    "    ##################################\n",
    "    ####          TODO            ####\n",
    "    ##################################\n",
    "    Y = 1/(np.exp(-Z) + 1)\n",
    "  # Y = (-np.exp(-Z) + np.exp(Z))/(np.exp(-Z) + np.exp(Z))\n",
    "    ##################################\n",
    "    ####          TODO            ####\n",
    "    ##################################\n",
    "    cache = Z\n",
    "\n",
    "    return Y, cache\n",
    "\n",
    "def sigmoid_backward(dY, cache):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    dY -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    ##################################\n",
    "    ####          TODO            ####\n",
    "    ##################################\n",
    "    dZ = (1/(np.exp(-Z) + 1))*(1 - (1/(np.exp(-Z) + 1)))*dY\n",
    "#   dZ = (1-((-np.exp(-Z) + np.exp(Z))/(np.exp(-Z) + np.exp(Z)))**2)*dY\n",
    "    ##################################\n",
    "    ####          TODO            ####\n",
    "    ##################################\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CAhROpvSiZmY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y : [0.11920292 0.26894142 0.5        0.73105858 0.88079708]\n",
      "dZ : [0.05249679 0.09830597 0.125      0.09830597 0.05249679]\n"
     ]
    }
   ],
   "source": [
    "# check sigmoid and sigmoid_backward\n",
    "Z = np.array([-2., -1., 0., 1., 2.])\n",
    "Y, cache = sigmoid(Z)\n",
    "dZ = sigmoid_backward(0.5, cache)\n",
    "print('Y : ' + str(Y))\n",
    "print('dZ : ' + str(dZ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lSxILcZgjwNv"
   },
   "source": [
    "↑  預期輸出:\n",
    "```\n",
    "Y : [0.11920292 0.26894142 0.5        0.73105858 0.88079708]\n",
    "dZ : [0.05249679 0.09830597 0.125      0.09830597 0.05249679]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tROH23uG14PA"
   },
   "source": [
    "# Loss or Cost\n",
    "象徵網路的性能(通常是表示有多差，數值越小越好)，求其梯度方向來做為優化參數的方向(梯度反向)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TkmIEuIn2kX3"
   },
   "source": [
    "## Cross Entropy\n",
    "+ define\n",
    "\n",
    "  $J(W) = CE(P, Y) = \\frac{\\sum_{i=1}^{n}(-y_i\\log(p_i) - (1-y_i)\\log(1- p_i))}{n}$\n",
    "\n",
    "+ gradient\n",
    "\n",
    "  $\\frac{dJ}{dP}= -\\frac{Y}{P}+\\frac{1-Y}{1-P}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的 log 指的是數學式的 ln"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gr0qcWs5mqMA"
   },
   "source": [
    "↓ 可參考: [numpy.dot](https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html) ,[numpy.ndarray.T](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.T.html) ,[numpy.log](https://docs.scipy.org/doc/numpy/reference/generated/numpy.log.html) ,[numpy.divide](https://docs.scipy.org/doc/numpy/reference/generated/numpy.divide.html) , etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ivtB7sKtxWVK"
   },
   "outputs": [],
   "source": [
    "def cost(P, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    P -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- Cross Entrophy loss\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[-1]\n",
    "    \n",
    "    # Compute loss from P and Y.\n",
    "    ##################################\n",
    "    ####          TODO            ####\n",
    "    ##################################\n",
    "    cost = -(np.dot(Y,np.log(P).T) + np.dot(1-Y,np.log(1-P).T))/m\n",
    "#   cost = -0.5 * (np.dot(log(1-Y),(1-P).T) + np.dot(log(1+Y),(1+P).T)) + log(2)\n",
    "    ##################################\n",
    "    ####          TODO            ####\n",
    "    ##################################\n",
    "\n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost\n",
    "\n",
    "def cost_grad(P, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    P -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    grad -- Gradient of the cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[-1]\n",
    "\n",
    "    # Compute gradient of the cost from P and Y.\n",
    "    ##################################\n",
    "    ####          TODO            ####\n",
    "    ##################################\n",
    "    grad = -(Y/P - (1-Y)/(1-P))\n",
    "#   grad = -.5*((1+P)/(1+Y) - (1-P)/(1-Y))\n",
    "    ##################################\n",
    "    ####          TODO            ####\n",
    "    ##################################  \n",
    "    return grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "im7lWkR4qIjq",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CE loss : 0.3993\n",
      "gradients : [[ 2.         -1.11111111]]\n"
     ]
    }
   ],
   "source": [
    "# check CE and gradient of it\n",
    "P = np.array([[0.5, .9]])\n",
    "Y = np.array([[0.0, 1.0]])\n",
    "loss = cost(P,Y)\n",
    "grad = cost_grad(P,Y)\n",
    "\n",
    "print('CE loss : %.4f' % loss)\n",
    "print('gradients : ' + str(grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2sPHO-Ic6nrQ"
   },
   "source": [
    "↑  預期輸出:\n",
    "```\n",
    "CE loss : 0.3993\n",
    "gradients : [[ 2.         -1.11111111]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5VHylwvQm8mn"
   },
   "source": [
    "# Perceptron\n",
    "![](https://i.imgur.com/3kbN5FW.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pSDjg0auD5Vl"
   },
   "outputs": [],
   "source": [
    "# fully one layer perceptron forward\n",
    "def layer_forward(X, W, b):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    \n",
    "    Returns:\n",
    "    Y -- the output of the activation function\n",
    "    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
    "            stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    ##################################\n",
    "    ####          TODO            ####\n",
    "    ##################################\n",
    "    Z, linear_cache = linear_forward(X, W, b)\n",
    "    Y, activation_cache = sigmoid(Z)\n",
    "    ##################################\n",
    "    ####          TODO            ####\n",
    "    ##################################\n",
    "\n",
    "    cache = (linear_cache, activation_cache)\n",
    "    return Y, cache\n",
    "\n",
    "# fully one layer perceptron backward\n",
    "def layer_backward(dY, cache):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    dY -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    \n",
    "    Returns:\n",
    "    dX -- Gradient of the cost with respect to the activation (of the previous layer l-1)\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    ##################################\n",
    "    ####          TODO            ####\n",
    "    ##################################\n",
    "    dZ = sigmoid_backward(dY, activation_cache)\n",
    "    dX, dW, db = linear_backward(dZ, linear_cache)\n",
    "    ##################################\n",
    "    ####          TODO            ####\n",
    "    ##################################\n",
    "    return dX, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2jU29fy6tA0t"
   },
   "source": [
    "## Multi-layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ojpl1hDJrAUE"
   },
   "outputs": [],
   "source": [
    "# define our model\n",
    "class MultiLayerModel():\n",
    "  def __init__(self, layers_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layers_dims --  list containing the input size and each layer size\n",
    "    \"\"\"\n",
    "    assert len(layers_dims)>=2\n",
    "    self.layers_num = len(layers_dims)\n",
    "\n",
    "    # initial parameters\n",
    "    self.parameters = {}\n",
    "    for l in range(1, self.layers_num):\n",
    "      self.parameters['W'+str(l)] = np.random.randn(layers_dims[l], layers_dims[l-1])\n",
    "      self.parameters['b'+str(l)] = np.zeros((layers_dims[l], 1))\n",
    "      # ex: self.parameters is {'W1': matrix[300, 784], 'b1':matrix[300,1], 'W2': matrix[1, 300], 'b2':matrix[1,1]} \n",
    "\n",
    "  def train(self, X, Y, learning_rate = 0.0075, num_iterations = 800):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X --  data, numpy array of shape (num_px * num_px, number of examples)\n",
    "    Y --  label, numpy array of shape (1, number of examples)\n",
    "    learning_rate -- learning rate of the optimization method(SGD)\n",
    "    num_iterations -- number of iterations of the train loop\n",
    "\n",
    "    Returns:\n",
    "    log -- tuple of values (loss, acc) we store record of evaluation indicators\n",
    "    \"\"\"\n",
    "    log = {'loss':[], 'acc':[]}\n",
    "    for i in range(num_iterations):\n",
    "      outputs = {0:X} # dictionary to store output of each layers\n",
    "      caches = {} # dictionary to store cache of each layers for computing backward propagation \n",
    "      grads = {} # dictionary to store gradient of the cost with respect to each parameters \n",
    "      # forward\n",
    "      for l in range(1, self.layers_num):\n",
    "        outputs[l], caches[l] = layer_forward(outputs[l-1], self.parameters['W'+str(l)], self.parameters['b'+str(l)])\n",
    "\n",
    "      # comput loss,accuracy  and gradient of loss for backwward\n",
    "      current_loss = cost(outputs[self.layers_num-1], Y)\n",
    "      acc = self.accuracy(outputs[self.layers_num-1], Y)\n",
    "      grads['Y'+str(self.layers_num-1)] = cost_grad(outputs[self.layers_num-1], Y)\n",
    "      \n",
    "      # backward\n",
    "      for l in range(self.layers_num-1, 0, -1):\n",
    "        grads['Y'+str(l-1)], grads['W'+str(l)], grads['b'+str(l)] = layer_backward(grads['Y'+str(l)], caches[l])\n",
    "\n",
    "      # update parameters (by SGD)\n",
    "      for l in range(1, self.layers_num):\n",
    "        self.parameters['W'+str(l)] += -1 * learning_rate * grads['W'+str(l)]\n",
    "        self.parameters['b'+str(l)] += -1 * learning_rate * grads['b'+str(l)]\n",
    "      \n",
    "      if (i%100) == 0:\n",
    "        print('[Epoch %d] [Loss : %.6f] [Acc : %3d%%]' % (i, current_loss, 100*acc))\n",
    "      log['loss'].append(current_loss)\n",
    "      log['acc'].append(acc)\n",
    "\n",
    "    return log\n",
    "\n",
    "  def predict(self, X):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X --  data, numpy array of shape (num_px * num_px, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    Y --  predict output, numpy array of shape (1, number of examples)\n",
    "    \"\"\"\n",
    "    Y = X\n",
    "    for l in range(1, self.layers_num):\n",
    "        Y, _ = layer_forward(Y, self.parameters['W'+str(l)], self.parameters['b'+str(l)])\n",
    "    return np.around(Y)\n",
    "\n",
    "  def accuracy(self, P, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    P --  predict output, numpy array of shape (1, number of examples)\n",
    "    Y --  label, numpy array of shape (1, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    acc --  accuracy by compare predict an label, float\n",
    "    \"\"\"\n",
    "    acc = np.sum(np.squeeze(np.around(P) == Y)) / Y.shape[-1]\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TdIQSn5tHG_K"
   },
   "outputs": [],
   "source": [
    "# create our model\n",
    "np.random.seed(109)\n",
    "layers_dims = [784\n",
    "               , 315 # you can change this number(number of neurons in hidden layer)\n",
    "               , 1]\n",
    "model = MultiLayerModel(layers_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sXlsQ-NDQUWo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] [Loss : 4.097987] [Acc :  49%]\n",
      "[Epoch 100] [Loss : 1.706520] [Acc :  53%]\n",
      "[Epoch 200] [Loss : 1.260342] [Acc :  63%]\n",
      "[Epoch 300] [Loss : 1.003502] [Acc :  69%]\n",
      "[Epoch 400] [Loss : 0.839400] [Acc :  74%]\n",
      "[Epoch 500] [Loss : 0.721977] [Acc :  79%]\n",
      "[Epoch 600] [Loss : 0.631053] [Acc :  82%]\n",
      "[Epoch 700] [Loss : 0.557316] [Acc :  83%]\n"
     ]
    }
   ],
   "source": [
    "# train our model\n",
    "log = model.train(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T_31CarEIiTF"
   },
   "source": [
    "↑  預期輸出(範例):\n",
    "```\n",
    "[Epoch 0] [Loss : 3.074105] [Acc :  54%]\n",
    "[Epoch 100] [Loss : 1.740730] [Acc :  60%]\n",
    "[Epoch 200] [Loss : 1.392251] [Acc :  66%]\n",
    ".\n",
    ".\n",
    ".\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z_SDJFOWtYXs"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the loss\n",
    "plt.plot(np.squeeze(log['loss']))\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('iterations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "30sHY2YgJNnj"
   },
   "source": [
    "↑  預期輸出(範例):\n",
    "![](https://i.imgur.com/u7NpsAp.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gRN_cu4Cad26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy : 79.0000%\n"
     ]
    }
   ],
   "source": [
    "# check this model is work when testing\n",
    "p_test = model.predict(x_test)\n",
    "acc = model.accuracy(p_test, y_test)*100\n",
    "print('test accuracy : %.4f%%' % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UcMwDpZFIsCL"
   },
   "source": [
    "↑  預期輸出(範例):\n",
    "```\n",
    "test accuracy : 72.0000%\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOurMjhorTGoFn0V1jsB+EO",
   "collapsed_sections": [],
   "name": "HW1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
